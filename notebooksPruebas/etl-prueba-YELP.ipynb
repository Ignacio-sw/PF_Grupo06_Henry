{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "import json\n",
    "import logging\n",
    "\n",
    "@functions_framework.http\n",
    "def ETL_reviews_bigquery(request):\n",
    "    try:\n",
    "        # Obtén la carpeta en el bucket desde los parámetros de la URL\n",
    "        estado = request.args.get('estado')\n",
    "\n",
    "        # Verifica si se proporcionó la carpeta en la URL\n",
    "        if not estado:\n",
    "            return \"Error: El parámetro 'estado' es necesario en la URL\", 400\n",
    "\n",
    "        # Define el nombre del bucket y la ruta de la carpeta temporal\n",
    "        bucket_name = 'data_lake_pf_henry'\n",
    "        bucket_path = f'Google Maps/reviews-estados/{estado}/'\n",
    "        temporal_folder_path = 'temporal/'\n",
    "\n",
    "        # Crea un cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtiene el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Lista todos los blobs en la carpeta\n",
    "        blobs = bucket.list_blobs(prefix=bucket_path)\n",
    "\n",
    "        # Inicializa una lista para almacenar los DataFrames de cada archivo\n",
    "        dfs = []\n",
    "\n",
    "        # Itera sobre cada archivo en la carpeta\n",
    "        for blob in blobs:\n",
    "            # Descarga el contenido del blob como un string\n",
    "            json_string = blob.download_as_text()\n",
    "\n",
    "            # Dividir el string por nuevas líneas y cargar cada objeto JSON\n",
    "            json_objects = [json.loads(j) for j in json_string.splitlines()]\n",
    "\n",
    "            # Convierte la lista de objetos JSON en un DataFrame de Pandas\n",
    "            df = pd.DataFrame(json_objects)\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Concatena todos los DataFrames en uno solo\n",
    "        df_combined = pd.concat(dfs)\n",
    "\n",
    "        #ETL\n",
    "        for columna in df_combined.columns:\n",
    "            if df_combined[columna].dtype == 'object':\n",
    "                # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                df_combined[columna] = df_combined[columna].fillna(\"No data\").str.strip()\n",
    "            elif df_combined[columna].dtype in ['int64', 'float64']:\n",
    "                # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                df_combined[columna] = df_combined[columna].fillna(0)\n",
    "        \n",
    "        # Eliminar duplicados\n",
    "        df_combined.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "        # Guarda el DataFrame combinado como un archivo CSV en Cloud Storage en la carpeta temporal\n",
    "        csv_file_path = 'gs://{}/{}'.format(bucket_name, temporal_folder_path + 'combined_data.json')\n",
    "        df_combined.to_json(csv_file_path, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "        #CARGA EN BIG QUERY:\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        # Especifica el nombre del conjunto de datos y la tabla\n",
    "        proyect_id='proyecto-final-henry-412703'\n",
    "        dataset_id = 'reviews_estados'  # Reemplaza 'proyecto-final-henry-412703' con tu ID de proyecto\n",
    "        table_id = estado  # Puedes cambiar esto según tus preferencias\n",
    "        full_table_id = f'{proyect_id}.{dataset_id}.{table_id}'\n",
    "        \n",
    "        # Configura el job de carga con autodetección de esquema\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            autodetect=True,  # Permite a BigQuery autodetectar el esquema\n",
    "            source_format=\"NEWLINE_DELIMITED_JSON\",\n",
    "            create_disposition=\"CREATE_IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        # Inicia el job de carga\n",
    "        load_job = client.load_table_from_uri(csv_file_path, full_table_id, job_config=job_config)\n",
    "\n",
    "        # Espera a que el job se complete\n",
    "        load_job.result()\n",
    "\n",
    "        # Devuelve la respuesta HTTP con el JSON del DataFrame\n",
    "        return \"Carga exitosa!\", 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en la función: {e}\", exc_info=True)\n",
    "        return f\"Error: {e}\", 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL archivo business.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def archivo_df_business(bucket_path):\n",
    "#Filtrado segun nombre archivo para su respectivo ETL\n",
    "            df_bussines=pd.read_pickle(bucket_path)\n",
    "            df_bussines.to_csv(bucket_path, index=False)\n",
    "            df_bussines=pd.read_csv(bucket_path)\n",
    "            df_bussines.drop(columns=['attributes', 'hours','postal_code','business_id.1', 'name.1',\n",
    "            'address.1', 'city.1', 'state.1', 'postal_code.1', 'latitude.1',\n",
    "            'longitude.1', 'stars.1', 'review_count.1', 'is_open.1', 'attributes.1',\n",
    "            'categories.1', 'hours.1'], inplace=True)\n",
    "            for columna in df_bussines.columns:\n",
    "                if df_bussines[columna].dtype == 'object':\n",
    "                    # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                    df_bussines[columna] = df_bussines[columna].fillna(\"No data\").str.strip()\n",
    "                elif df_bussines[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                    df_bussines[columna] = df_bussines[columna].fillna(0)\n",
    "            return (df_bussines)\n",
    "def archivo_df_tip (bucket_path):\n",
    "    df_tip=pd.read_json(bucket_path, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    return (df_tip)\n",
    "def archivo_df_user(bucket_path):\n",
    "    df_user001=pd.read_parquet(bucket_path)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    return (df_user001)\n",
    "\n",
    "@functions_framework.http\n",
    "def ETL_reviews_bigquery(request):\n",
    "    try:\n",
    "        # Obtén la carpeta en el bucket desde los parámetros de la URL\n",
    "        archivo = request.args.get('archivo')\n",
    "\n",
    "        # Verifica si se proporcionó la carpeta en la URL\n",
    "        if not archivo:\n",
    "            return \"Error: El parámetro 'archivo' es necesario en la URL\", 400\n",
    "\n",
    "        # Define el nombre del bucket y la ruta de la carpeta temporal\n",
    "        bucket_name = 'data_lake_pf_henry'\n",
    "        bucket_path = f'YELP/Yelp/{archivo}/'\n",
    "        temporal_folder_path = 'temporal/'\n",
    "\n",
    "        # Crea un cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtiene el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Lista todos los blobs en la carpeta\n",
    "        blobs = bucket.list_blobs(prefix=bucket_path)\n",
    "        \n",
    "        if \"business\" in bucket_path:\n",
    "            df_combined=archivo_df_business(bucket_path)\n",
    "        if \"tip\" in bucket_path:\n",
    "            df_combined=archivo_df_tip(bucket_path)\n",
    "        if \"user-001\" in bucket_path:\n",
    "            df_combined=archivo_df_user(bucket_path)\n",
    "        \n",
    "\n",
    "        # Guarda el DataFrame combinado como un archivo CSV en Cloud Storage en la carpeta temporal\n",
    "        csv_file_path = 'gs://{}/{}'.format(bucket_name, temporal_folder_path + 'combined_data.json')\n",
    "        df_combined.to_json(csv_file_path, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "        #CARGA EN BIG QUERY:\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        # Especifica el nombre del conjunto de datos y la tabla\n",
    "        proyect_id='proyecto-final-henry-412703'\n",
    "        dataset_id = 'reviews_archivos'  # Reemplaza 'proyecto-final-henry-412703' con tu ID de proyecto\n",
    "        table_id = archivo  # Puedes cambiar esto según tus preferencias\n",
    "        full_table_id = f'{proyect_id}.{dataset_id}.{table_id}'\n",
    "        \n",
    "        # Configura el job de carga con autodetección de esquema\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            autodetect=True,  # Permite a BigQuery autodetectar el esquema\n",
    "            source_format=\"NEWLINE_DELIMITED_JSON\",\n",
    "            create_disposition=\"CREATE_IF_NEEDED\"\n",
    "        )\n",
    "\n",
    "        # Inicia el job de carga\n",
    "        load_job = client.load_table_from_uri(csv_file_path, full_table_id, job_config=job_config)\n",
    "\n",
    "        # Espera a que el job se complete\n",
    "        load_job.result()\n",
    "\n",
    "        # Devuelve la respuesta HTTP con el JSON del DataFrame\n",
    "        return \"Carga exitosa!\", 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en la función: {e}\", exc_info=True)\n",
    "        return f\"Error: {e}\", 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archivo_df_review(bucket_path):\n",
    "    df_review=pd.read_json(bucket_path, lines=True)\n",
    "    data_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
