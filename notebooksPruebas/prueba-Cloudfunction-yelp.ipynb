{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "# categorías a usar\n",
    "categories = ['No Category','Restaurant', 'Bar', 'Pub', 'Cafe', 'Bakery', 'Coffee shop', 'Gym', 'Gas station']\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "\n",
    "  data = cloud_event.data\n",
    "\n",
    "  event_id = cloud_event[\"id\"]\n",
    "  event_type = cloud_event[\"type\"]\n",
    "  print(f'Cloud event ID: {event_id}')\n",
    "  print(f'Event type: {event_type}')\n",
    "\n",
    "  bucket_name = data[\"bucket\"]\n",
    "  name = data[\"name\"]\n",
    "  print(f'Bucket: {bucket_name}')\n",
    "  print(f'File name: {name}')\n",
    "  # Create URI\n",
    "  file_uri = f'gs://{bucket_name}/{name}'\n",
    "  print(f'URI: {file_uri}')\n",
    "\n",
    "  # Obtener el objeto del bucket\n",
    "  bucket = cloud_storage.bucket(bucket_name)\n",
    "\n",
    "  # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "  blob = bucket.blob(name)\n",
    "\n",
    "\n",
    "  # Metodo blob.open()\n",
    "  # with blob.open(mode='r', encoding='utf-8') as f:\n",
    "    # Dividir el string por nuevas líneas y cargar cada objeto JSON\n",
    "    # json_objects = [json.loads(j) for j in json_string.splitlines()]\n",
    "  #   objects = f.read()\n",
    "  #   print(type(objects))\n",
    "  #   print(len(objects))\n",
    "\n",
    "  # Attempt to create DataFrame with URI\n",
    "  try:\n",
    "    df_uri = pd.read_json(\n",
    "      file_uri,\n",
    "      lines=True\n",
    "    )\n",
    "    # If DataFrame is loaded, the print a success message\n",
    "    print(f'Succeed: DataFrame loaded from {file_uri}')\n",
    "    print(f'Resulting shape: {df_uri.shape}')\n",
    "    print(f'DataFrame Columns: {df_uri.columns}')\n",
    "  except Exception as e:\n",
    "    # Otherwise, catch the error\n",
    "    print('Error: Could not read from URI. -', e)\n",
    "\n",
    "  # Attempt to process DataFrame\n",
    "  print('Transformando dataframe...')\n",
    "  try:\n",
    "    # Procesar data con las categorías definidas\n",
    "    df_procesado, df_misc, df_categorias = procesar(df_uri, categories)\n",
    "\n",
    "    # Crear hash de categorias\n",
    "    df_procesado['category_id'] = df_procesado['category'].apply(hash_category, df_categories=df_categorias)\n",
    "    df_procesado = df_procesado.drop(columns='category')\n",
    "\n",
    "    # Reordenar columnas\n",
    "    cols = ['gmap_id', 'name', 'address', 'description', 'category_id', 'avg_rating',\n",
    "            'num_of_reviews', 'latitude', 'longitude', 'relative_results', 'url' ]\n",
    "    df_procesado = df_procesado[cols]\n",
    "\n",
    "    # Si procesa la función correctamente\n",
    "    print('DataFrame Procesado')\n",
    "    print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    print('Tabla MISC desagregada, shape:', df_misc.shape)\n",
    "    print('Tabla categoria desagregada, shape:', df_categorias.shape)\n",
    "\n",
    "  except Exception as e:\n",
    "    # Cuando no procesa la información\n",
    "    print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "    return None\n",
    "    \n",
    "\n",
    "  # Create DataFrame\n",
    "  # df = pd.DataFrame(\n",
    "  #   [\n",
    "  #     {'id': 1, 'name': 'Pepito', 'last_name': 'Pérez', 'message': 'Esto'},\n",
    "  #     {'id': 2, 'name': 'Pepita', 'last_name': 'Pérez', 'message': 'es'},\n",
    "  #     {'id': 3, 'name': 'Juan', 'last_name': 'Pérez', 'message': 'una'},\n",
    "  #     {'id': 4, 'name': 'Roberto', 'last_name': 'Pérez', 'message': 'prueba'},\n",
    "  #   ]\n",
    "  # )\n",
    "\n",
    "  # Insert to table\n",
    "  table_id = 'dms-pfh.google.google-sites'\n",
    "\n",
    "  # Create job configurations\n",
    "  job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    create_disposition='CREATE_IF_NEEDED',\n",
    "    write_disposition='WRITE_APPEND'\n",
    "    # source_format='NEWLINE_DELIMITED_JSON'\n",
    "  )\n",
    "  print('Job configuration created.')\n",
    "\n",
    "  # Requesting the job\n",
    "  load_job = bq.load_table_from_dataframe(\n",
    "    df_procesado,\n",
    "    table_id,\n",
    "    location='us-central1',\n",
    "    job_config=job_config\n",
    "  )\n",
    "  load_job.result()\n",
    "  print('Job finished')\n",
    "\n",
    "  # Requesting a second job\n",
    "  table_id_misc = 'dms-pfh.google.misc-dim'\n",
    "  load_job2 = bq.load_table_from_dataframe(\n",
    "    df_misc,\n",
    "    table_id_misc,\n",
    "    location='us-central1',\n",
    "    job_config=job_config\n",
    "  )\n",
    "  load_job2.result()\n",
    "  print('Job 2 finished; MISC dim ingested')\n",
    "\n",
    "\n",
    "  # Requesting a third job\n",
    "\n",
    "  # Particular jobConfig\n",
    "  job_config_2= bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    create_disposition='CREATE_IF_NEEDED',\n",
    "    write_disposition='WRITE_EMPTY'\n",
    "    # source_format='NEWLINE_DELIMITED_JSON'\n",
    "  )\n",
    "\n",
    "  table_id_cat = 'dms-pfh.google.site-categories'\n",
    "  load_job3 = bq.load_table_from_dataframe(\n",
    "    df_categorias,\n",
    "    table_id_cat,\n",
    "    location='us-central1',\n",
    "    job_config=job_config_2\n",
    "  )\n",
    "  load_job3.result()\n",
    "  print('Categories dim: OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "#Filtrado segun nombre archivo para su respectivo ETL\n",
    "            df_bussines=pd.read_pickle(file_uri)\n",
    "            df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "\n",
    "            df_bussines.drop(columns=['attributes', 'hours','postal_code'], inplace=True)\n",
    "            for columna in df_bussines.columns:\n",
    "                if df_bussines[columna].dtype == 'object':\n",
    "                    # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                    df_bussines[columna] = df_bussines[columna].fillna(\"No data\").str.strip()\n",
    "                elif df_bussines[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                    df_bussines[columna] = df_bussines[columna].fillna(0)\n",
    "            categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "            df_business = df_business[df_business['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "            df_mascara= df_business[\"business_id\"]\n",
    "\n",
    "            return (df_bussines)\n",
    "def archivo_df_tip (file_uri):\n",
    "    df_tip=pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "                if df_tip[columna].dtype == 'object':\n",
    "                    # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                    df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "                elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                    df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return (df_tip)\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001=pd.read_parquet(file_uri)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "                if df_user001[columna].dtype == 'object':\n",
    "                    # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                    df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "                elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                    df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    return (df_user001)\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    datos_review=[]\n",
    "    # Abrir el archivo JSON y leer cada línea\n",
    "    with open(file_uri, 'r', encoding='utf-8') as archivo:\n",
    "      for linea in archivo:\n",
    "          # Intentar cargar el objeto JSON de la línea\n",
    "          try:\n",
    "              objeto_json = json.loads(linea)\n",
    "              datos_review.append(objeto_json)\n",
    "          except json.JSONDecodeError as e:\n",
    "              print(f\"Error al decodificar JSON en línea: {e}\")\n",
    "    # Crear un DataFrame a partir de los datos\n",
    "    df_review = pd.DataFrame(datos_review)\n",
    "    for columna in df_review.columns:\n",
    "                if df_review[columna].dtype == 'object':\n",
    "                    # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "                    df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "                elif df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "                    df_review[columna] = df_review[columna].fillna(0)\n",
    "    return(df_review)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "\n",
    "  data = cloud_event.data\n",
    "\n",
    "  event_id = cloud_event[\"id\"]\n",
    "  event_type = cloud_event[\"type\"]\n",
    "  print(f'Cloud event ID: {event_id}')\n",
    "  print(f'Event type: {event_type}')\n",
    "\n",
    "  bucket_name = data[\"bucket\"]\n",
    "  name = data[\"name\"]\n",
    "  print(f'Bucket: {bucket_name}')\n",
    "  print(f'File name: {name}')\n",
    "  # Create URI\n",
    "  file_uri = f'gs://{bucket_name}/{name}'\n",
    "  print(f'URI: {file_uri}')\n",
    "\n",
    "  # Obtener el objeto del bucket\n",
    "  bucket = cloud_storage.bucket(bucket_name)\n",
    "\n",
    "  # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "  blob = bucket.blob(name)\n",
    "  # Attempt to create DataFrame with URI\n",
    "\n",
    "  # Attempt to process DataFrame\n",
    "  print('Transformando dataframe...')\n",
    "  try:\n",
    "    if \"business\" in file_uri:\n",
    "            df_procesado=archivo_df_business(file_uri)\n",
    "            table_name=\"yelp-sites\" \n",
    "    if \"tip\" in file_uri:\n",
    "            df_procesado=archivo_df_tip(file_uri)\n",
    "            table_name=\"yelp-user-tips\"\n",
    "    if \"user-001\" in file_uri:\n",
    "            df_procesado=archivo_df_user(file_uri)\n",
    "            table_name=\"yelp-user-info\"\n",
    "    if \"review-002\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('DataFrame Procesado')\n",
    "    print('Nuevas dimensiones:', df_procesado.shape)\n",
    "\n",
    "  except Exception as e:\n",
    "    # Cuando no procesa la información\n",
    "    print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "    return None\n",
    "\n",
    "  # Insert to table\n",
    "  table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "\n",
    "  # Create job configurations\n",
    "  job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    create_disposition='CREATE_IF_NEEDED',\n",
    "    write_disposition='WRITE_APPEND'\n",
    "    # source_format='NEWLINE_DELIMITED_JSON'\n",
    "  )\n",
    "  print('Job configuration created.')\n",
    "\n",
    "  # Requesting the job\n",
    "  load_job = bq.load_table_from_dataframe(\n",
    "    df_procesado,\n",
    "    table_id,\n",
    "    location='us-central1',\n",
    "    job_config=job_config\n",
    "  )\n",
    "  load_job.result()\n",
    "  print('Job finished')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo que anda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "#from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "    # Crear una instancia del cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Nombre del archivo en el bucket\n",
    "    nombre_archivo = 'df_mascara.csv'\n",
    "\n",
    "    # Nombre del bucket\n",
    "    nombre_bucket = 'yelp_dms'\n",
    "    # Ruta completa en el bucket\n",
    "    ruta_archivo = f'gs://{nombre_bucket}/{nombre_archivo}'\n",
    "    \n",
    "    # Filtrado segun nombre archivo para su respectivo ETL\n",
    "    df_bussines = pd.read_pickle(file_uri)\n",
    "    df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "    print(\"pase la mascara de juan de columnas\")\n",
    "    df_bussines.drop(columns=[\"is_open\",'attributes', 'hours','postal_code'], inplace=True)\n",
    "    categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "\n",
    "    # Eliminar filas con valores NaN en la columna 'categories'\n",
    "    df_bussines = df_bussines.fillna(\"No Data\")\n",
    "    # Filtrar el DataFrame para mantener solo las filas que contienen al menos una de las categorías deseadas\n",
    "    df_bussines = df_bussines[df_bussines['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "    df_mascara= df_bussines[\"business_id\"]\n",
    "\n",
    "    # Guardar el DataFrame como un archivo CSV\n",
    "    df_mascara.to_csv(nombre_archivo, index=False)\n",
    "    # Obtener el bucket\n",
    "    bucket = client.bucket(nombre_bucket)\n",
    "    # Subir el archivo al bucket\n",
    "    blob = bucket.blob(nombre_archivo)\n",
    "    blob.upload_from_filename(nombre_archivo)\n",
    "    print(f'DataFrame guardado en {ruta_archivo}')\n",
    "    return df_bussines\n",
    "\n",
    "def archivo_df_tip(file_uri):\n",
    "    df_tip = pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "        if df_tip[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return df_tip\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001 = pd.read_parquet(file_uri)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "        if df_user001[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    return df_user001\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    datos_review=[]\n",
    "    # Abrir el archivo JSON y leer cada línea\n",
    "    with open(file_uri, 'r', encoding='utf-8') as archivo:\n",
    "        for linea in archivo:\n",
    "              # Intentar cargar el objeto JSON de la línea\n",
    "            try:\n",
    "                objeto_json = json.loads(linea)\n",
    "                datos_review.append(objeto_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al decodificar JSON en línea: {e}\")\n",
    "    # Crear un DataFrame a partir de los datos\n",
    "    df_review = pd.DataFrame(datos_review)\n",
    "    for columna in df_review.columns:\n",
    "        if df_review[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_review[columna] = df_review[columna].fillna(0)\n",
    "    df_mascara=pd.read_csv(\"gs://yelp_dms/df_mascara.csv\")\n",
    "    df_review = pd.merge(df_review, df_mascara, on='business_id', how='inner')\n",
    "    print(\"filtracion completa\")\n",
    "    return df_review\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "    data = cloud_event.data\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "    bucket_name = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    print(f'Bucket: {bucket_name}')\n",
    "    print(f'File name: {name}')\n",
    "    # Create URI\n",
    "    file_uri = f'gs://{bucket_name}/{name}'\n",
    "    print(f'URI: {file_uri}')\n",
    "    # Obtener el objeto del bucket\n",
    "    bucket = cloud_storage.bucket(bucket_name)\n",
    "    # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "    blob = bucket.blob(name)\n",
    "    # Attempt to create DataFrame with URI\n",
    "    # Attempt to process DataFrame\n",
    "    print('Transformando dataframe...')\n",
    "    try:\n",
    "        if \"business\" in file_uri:\n",
    "            df_procesado = archivo_df_business(file_uri)\n",
    "            table_name = \"yelp-sites\"\n",
    "            print(\"dataframe cargado\") \n",
    "        if \"tip\" in file_uri:\n",
    "            df_procesado = archivo_df_tip(file_uri)\n",
    "            table_name = \"yelp-user-tips\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"user-001\" in file_uri:\n",
    "            df_procesado = archivo_df_user(file_uri)\n",
    "            table_name = \"yelp-user-info\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"review-002\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "            print(\"dataframe cargado\")\n",
    "        print('DataFrame Procesado')\n",
    "        print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    except Exception as e:\n",
    "        # Cuando no procesa la información\n",
    "        print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "        return None\n",
    "    # Insert to table\n",
    "    table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        create_disposition='CREATE_IF_NEEDED',\n",
    "        write_disposition='WRITE_APPEND'\n",
    "        # source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "    print('Job configuration created.')\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_dataframe(\n",
    "        df_procesado,\n",
    "        table_id,\n",
    "        location='us-central1',\n",
    "        job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    print('Job finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo que esta en la function que anda de yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "#from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "    # Crear una instancia del cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Nombre del archivo en el bucket\n",
    "    nombre_archivo = 'df_mascara.csv'\n",
    "\n",
    "    # Nombre del bucket\n",
    "    nombre_bucket = 'yelp_dms'\n",
    "    # Ruta completa en el bucket\n",
    "    ruta_archivo = f'gs://{nombre_bucket}/{nombre_archivo}'\n",
    "    \n",
    "    # Filtrado segun nombre archivo para su respectivo ETL\n",
    "    df_bussines = pd.read_pickle(file_uri)\n",
    "    df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "    print(\"pase la mascara de juan de columnas\")\n",
    "    df_bussines.drop(columns=[\"is_open\",'attributes', 'hours','postal_code'], inplace=True)\n",
    "    categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "\n",
    "    # Eliminar filas con valores NaN en la columna 'categories'\n",
    "    df_bussines = df_bussines.fillna(\"No Data\")\n",
    "    # Filtrar el DataFrame para mantener solo las filas que contienen al menos una de las categorías deseadas\n",
    "    df_bussines = df_bussines[df_bussines['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "    df_mascara= df_bussines[\"business_id\"]\n",
    "    estados_costa_este = [\"ME\", \"NH\", \"VT\", \"MA\", \"RI\", \"CT\", \"NY\", \"NJ\", \"PA\", \"MD\", \"DE\", \"WV\", \"VA\", \"NC\", \"SC\", \"GA\", \"FL\"]\n",
    "    # Filtrar df_mascara por los estados de la costa este\n",
    "    df_mascara= df_mascara[df_mascara.isin(estados_costa_este)]\n",
    "    print(\"la dimension filtrada de estados es \",df_mascara.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    # Guardar el DataFrame como un archivo CSV\n",
    "    df_mascara.to_csv(nombre_archivo, index=False)\n",
    "    # Obtener el bucket\n",
    "    bucket = client.bucket(nombre_bucket)\n",
    "    # Subir el archivo al bucket\n",
    "    blob = bucket.blob(nombre_archivo)\n",
    "    blob.upload_from_filename(nombre_archivo)\n",
    "    print(f'DataFrame guardado en {ruta_archivo}')\n",
    "    return df_bussines\n",
    "\n",
    "def archivo_df_tip(file_uri):\n",
    "    df_tip = pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "        if df_tip[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return df_tip\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001 = pd.read_parquet(file_uri)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "        if df_user001[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    return df_user001\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    datos_review=[]\n",
    "    # Abrir el archivo JSON y leer cada línea\n",
    "    with open(file_uri, 'r', encoding='utf-8') as archivo:\n",
    "        for linea in archivo:\n",
    "              # Intentar cargar el objeto JSON de la línea\n",
    "            try:\n",
    "                objeto_json = json.loads(linea)\n",
    "                datos_review.append(objeto_json)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al decodificar JSON en línea: {e}\")\n",
    "    # Crear un DataFrame a partir de los datos\n",
    "    df_review = pd.DataFrame(datos_review)\n",
    "    for columna in df_review.columns:\n",
    "        if df_review[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_review[columna] = df_review[columna].fillna(0)\n",
    "    df_mascara=pd.read_csv(\"gs://yelp_dms/df_mascara.csv\")\n",
    "    df_review = pd.merge(df_review, df_mascara, on='business_id', how='inner')\n",
    "    print(\"filtracion completa\")\n",
    "    return df_review\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "    data = cloud_event.data\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "    bucket_name = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    print(f'Bucket: {bucket_name}')\n",
    "    print(f'File name: {name}')\n",
    "    # Create URI\n",
    "    file_uri = f'gs://{bucket_name}/{name}'\n",
    "    print(f'URI: {file_uri}')\n",
    "    # Obtener el objeto del bucket\n",
    "    bucket = cloud_storage.bucket(bucket_name)\n",
    "    # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "    blob = bucket.blob(name)\n",
    "    # Attempt to create DataFrame with URI\n",
    "    # Attempt to process DataFrame\n",
    "    print('Transformando dataframe...')\n",
    "    try:\n",
    "        if \"business\" in file_uri:\n",
    "            df_procesado = archivo_df_business(file_uri)\n",
    "            table_name = \"yelp-sites\"\n",
    "            print(\"dataframe cargado\") \n",
    "        if \"tip\" in file_uri:\n",
    "            df_procesado = archivo_df_tip(file_uri)\n",
    "            table_name = \"yelp-user-tips\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"user-001\" in file_uri:\n",
    "            df_procesado = archivo_df_user(file_uri)\n",
    "            table_name = \"yelp-user-info\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"review-002\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "            print(\"dataframe cargado\")\n",
    "        print('DataFrame Procesado')\n",
    "        print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    except Exception as e:\n",
    "        # Cuando no procesa la información\n",
    "        print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "        return None\n",
    "    # Insert to table\n",
    "    table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        create_disposition='CREATE_IF_NEEDED',\n",
    "        write_disposition='WRITE_APPEND'\n",
    "        # source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "    print('Job configuration created.')\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_dataframe(\n",
    "        df_procesado,\n",
    "        table_id,\n",
    "        location='us-central1',\n",
    "        job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    print('Job finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo que esta en la nube que anda google mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
