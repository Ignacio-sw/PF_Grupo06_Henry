{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import functions_framework\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo juan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_from_json(cloud_event):\n",
    "\n",
    "    data = cloud_event.data\n",
    "\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "\n",
    "    # 1st. Get the file URI\n",
    "    bucket = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    # Create URI for ingestion\n",
    "    file_uri = f'gs://{bucket}/{name}'\n",
    "---------------------------------------\n",
    "\n",
    "---------------------------------------\n",
    "    # Insert into Bigquery ----\n",
    "    # Create a table id\n",
    "    table_id = f'{project_id}.raw_revs.reviews'\n",
    "\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "      autodetect=True,\n",
    "      source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_uri(\n",
    "      file_uri,\n",
    "      table_id,\n",
    "      location='us-central1',\n",
    "      job_config=job_config\n",
    "    )\n",
    "\n",
    "    load_job.result()\n",
    "\n",
    "    destination_table = bq.get_table(table_id)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtrado por la mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id=r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\REVIEW-ESTADOS\\reviews-estados\\review-Alabama\\1.json\"\n",
    "\n",
    "\n",
    "def ETL_reviews(df_json):\n",
    "    df=df_json.copy()\n",
    "    #eliminado de columnas PICS y RESP\n",
    "    df.drop(columns=[\"pics\", \"resp\"], inplace=True)\n",
    "    # Convertir la columna 'time' a formato de fecha y hora\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    # Proceso de ETL\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df[columna] = df[columna].fillna(\"No data\").str.strip()\n",
    "        elif df[columna].dtype in ['int64', 'float64']:\n",
    "        # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df[columna] = df[columna].fillna(0)\n",
    "        \n",
    "    # Eliminar duplicados basados en las columnas 'gmap_id' y 'user_id'\n",
    "    df.drop_duplicates(subset=['gmap_id', 'user_id'], inplace=True)\n",
    "    #cambiar tipo dato columna user_id a string\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filtracion(df_clean, df_mascara):\n",
    "    # Obtener los gmap_id del DataFrame df_final\n",
    "    gmap_ids_filtrar = df_mascara['gmap_id']\n",
    "\n",
    "    # Filtrar el DataFrame df_clean utilizando los gmap_id obtenidos\n",
    "    df_reviews = df_clean[df_clean['gmap_id'].isin(gmap_ids_filtrar)]\n",
    "\n",
    "    # Restablecer los índices del DataFrame resultante\n",
    "    df_reviews.reset_index(drop=True, inplace=True)\n",
    "    return (df_reviews)\n",
    "\n",
    "\n",
    "#table_id = ruta\n",
    "df=pd.read_json(table_id, lines=True)\n",
    "df_mascara=pd.read_csv(\"DIRECCION alojamiento DEL BUCKET DE LA MASCARA\")\n",
    "df_clean=ETL_reviews(df)\n",
    "df_filtrado=filtracion(df_clean, df_mascara)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo para la cracion de la mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def creacion_mascara():\n",
    "\n",
    "    todas_mascaras = []\n",
    "\n",
    "    # Iterar sobre los nombres de archivo del 1 al 11 inclusive\n",
    "    for i in range(1, 12):\n",
    "        # Construir la ruta del archivo\n",
    "        ruta = fr\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\GOOGLE\\DATA SITIOS\\metadata-sitios\\{i}.json\"\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        data = pd.read_json(ruta, lines=True)\n",
    "        \n",
    "        # Filtrar el DataFrame para las categorías permitidas\n",
    "        df_mascara = data[data['category'].apply(lambda x: isinstance(x, list) and any(cat in ['Restaurant', 'Bar', 'Pub', 'Cafe'] for cat in x))]\n",
    "        \n",
    "        # Seleccionar la columna 'gmap_id' del DataFrame filtrado\n",
    "        df_mascara = df_mascara[['gmap_id']]\n",
    "        \n",
    "        # Eliminar los valores duplicados de la columna 'gmap_id'\n",
    "        df_mascara_sin_duplicados = df_mascara.drop_duplicates(subset=['gmap_id'])\n",
    "        \n",
    "        # Agregar la máscara sin duplicados a la lista de todas las máscaras\n",
    "        todas_mascaras.append(df_mascara_sin_duplicados)\n",
    "\n",
    "    # Combinar todas las máscaras en un solo DataFrame\n",
    "    df_final = pd.concat(todas_mascaras, ignore_index=True)\n",
    "    return (df_final)\n",
    "\n",
    "mascara=creacion_mascara()\n",
    "mascara.to_csv(\"mascara.csv\", index=False)\n",
    "#aca se guardaria en el bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "# Crear función para la creación de la máscara\n",
    "def creacion_mascara():\n",
    "    todas_mascaras = []\n",
    "\n",
    "    # Iterar sobre los nombres de archivo del 1 al 11 inclusive\n",
    "    for i in range(1, 12):\n",
    "        # Construir la ruta del archivo\n",
    "        ruta = fr\"gs://data_lake_pf_henry/Google Maps/metadata-sitios/{i}.json\"\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        data = pd.read_json(ruta, lines=True)\n",
    "        \n",
    "        # Filtrar el DataFrame para las categorías permitidas\n",
    "        df_mascara = data[data['category'].apply(lambda x: isinstance(x, list) and any(cat in ['Restaurant', 'Bar', 'Pub', 'Cafe'] for cat in x))]\n",
    "        \n",
    "        # Seleccionar la columna 'gmap_id' del DataFrame filtrado\n",
    "        df_mascara = df_mascara[['gmap_id']]\n",
    "        \n",
    "        # Eliminar los valores duplicados de la columna 'gmap_id'\n",
    "        df_mascara_sin_duplicados = df_mascara.drop_duplicates(subset=['gmap_id'])\n",
    "        \n",
    "        # Agregar la máscara sin duplicados a la lista de todas las máscaras\n",
    "        todas_mascaras.append(df_mascara_sin_duplicados)\n",
    "\n",
    "    # Combinar todas las máscaras en un solo DataFrame\n",
    "    df_final = pd.concat(todas_mascaras, ignore_index=True)\n",
    "    return df_final\n",
    "\n",
    "# Crear la máscara de datos\n",
    "mascara = creacion_mascara()\n",
    "\n",
    "# Guardar la máscara en un archivo CSV en un bucket de Cloud Storage\n",
    "bucket_name = 'data_lake_pf_henry'\n",
    "blob_name = 'mascara.csv'\n",
    "bucket = cloud_storage.bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Convertir la máscara a formato CSV y guardarla en el blob\n",
    "mascara_csv = mascara.to_csv(\"mascara.csv\",index=False)\n",
    "blob.upload_from_string(mascara_csv, content_type='text/csv')\n",
    "\n",
    "print(f\"Máscara guardada correctamente en gs://{bucket_name}/{blob_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "from google.cloud.storage import Blob\n",
    "from google.cloud import functions_framework\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "@functions_framework.cloud_event\n",
    "def load_from_json(cloud_event):\n",
    "    # Get the file URI\n",
    "    data = cloud_event.data\n",
    "    bucket = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    file_uri = f'gs://{bucket}/{name}'\n",
    "\n",
    "    # Load JSON data from Cloud Storage\n",
    "    df = pd.read_json(file_uri, lines=True)\n",
    "    \n",
    "    # Clean and filter the data\n",
    "    df_clean = ETL_reviews(df)\n",
    "    df_filtered = filtracion(df_clean)\n",
    "    \n",
    "    # Save the filtered data back to Cloud Storage\n",
    "    bucket_name = 'bucket_name'\n",
    "    blob_name = 'mascara.csv'\n",
    "    blob = Blob(blob_name, cloud_storage.bucket(bucket_name))\n",
    "    df_filtered.to_csv(f\"gs://{bucket_name}/{blob_name}\", index=False)\n",
    "    \n",
    "    # Load data into BigQuery if needed\n",
    "    load_data_into_bigquery(df_filtered)\n",
    "\n",
    "\n",
    "def ETL_reviews(df):\n",
    "    # Your ETL process here\n",
    "    df=df.copy()\n",
    "    #eliminado de columnas PICS y RESP\n",
    "    df.drop(columns=[\"pics\", \"resp\"], inplace=True)\n",
    "    # Convertir la columna 'time' a formato de fecha y hora\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    # Proceso de ETL\n",
    "    \n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df[columna] = df[columna].fillna(\"No data\").str.strip()\n",
    "        elif df[columna].dtype in ['int64', 'float64']:\n",
    "        # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df[columna] = df[columna].fillna(0)\n",
    "        \n",
    "    # Eliminar duplicados basados en las columnas 'gmap_id' y 'user_id'\n",
    "    df.drop_duplicates(subset=['gmap_id', 'user_id'], inplace=True)\n",
    "    #cambiar tipo dato columna user_id a string\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filtracion(df_clean):\n",
    "    # Obtener los gmap_id del DataFrame df_final\n",
    "    df_mascara=pd.read_csv(\"gs://data_lake_pf_henry/Google Maps/mascara.csv\")\n",
    "    gmap_ids_filtrar = df_mascara['gmap_id']\n",
    "\n",
    "    # Filtrar el DataFrame df_clean utilizando los gmap_id obtenidos\n",
    "    df_reviews = df_clean[df_clean['gmap_id'].isin(gmap_ids_filtrar)]\n",
    "\n",
    "    # Restablecer los índices del DataFrame resultante\n",
    "    df_reviews.reset_index(drop=True, inplace=True)\n",
    "    return df_reviews\n",
    "\n",
    "\n",
    "def load_data_into_bigquery(df):\n",
    "    # Insert data into BigQuery\n",
    "    table_id = f'{project_id}.raw_revs.reviews'\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "    )\n",
    "    job = bq.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(\"Loaded {} rows into BigQuery table {}\".format(job.output_rows, table_id))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_from_json({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "#import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud.functions import storage\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "# Crear función para la creación de la máscara\n",
    "def creacion_mascara(event):\n",
    "    # 1. Obtener el nombre del bucket y el nombre del archivo\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    # 2. Verificar si el evento corresponde a un archivo CSV\n",
    "    if file_name.endswith('.csv'):\n",
    "        # 3. Leer el archivo CSV\n",
    "        blob = cloud_storage.bucket(bucket_name).blob(file_name)\n",
    "        content = blob.download_as_string()\n",
    "        df = pd.read_csv(content)\n",
    "\n",
    "        # 4. Filtrar el DataFrame para las categorías permitidas\n",
    "        df_mascara = df[df['category'].apply(lambda x: isinstance(x, list) and any(cat in ['Restaurant', 'Bar', 'Pub', 'Cafe'] for cat in x))]\n",
    "        \n",
    "        # 5. Seleccionar la columna 'gmap_id' del DataFrame filtrado\n",
    "        df_mascara = df_mascara[['gmap_id']]\n",
    "        \n",
    "        # 6. Eliminar los valores duplicados de la columna 'gmap_id'\n",
    "        df_mascara_sin_duplicados = df_mascara.drop_duplicates(subset=['gmap_id'])\n",
    "\n",
    "        # 7. Guardar la máscara en un archivo CSV en el mismo bucket\n",
    "        output_bucket = cloud_storage.bucket(bucket_name)\n",
    "        output_blob = output_bucket.blob('mascara.csv')\n",
    "        output_blob.upload_from_string(df_mascara_sin_duplicados.to_csv(index=False), content_type='text/csv')\n",
    "\n",
    "        print(f\"Máscara guardada correctamente en gs://{bucket_name}/mascara.csv\")\n",
    "\n",
    "# Establecer las credenciales de autenticación si es necesario\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"ruta_a_tus_credenciales.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    creacion_mascara({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nico codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import logging\n",
    "from pandas_gbq import to_gbq\n",
    "\n",
    "@functions_framework.http\n",
    "def ETL_reviews_bigquery(request):\n",
    "    try:\n",
    "        # Obtén la carpeta en el bucket desde los parámetros de la URL\n",
    "        estado = request.args.get('estado')\n",
    "\n",
    "        # Verifica si se proporcionó la carpeta en la URL\n",
    "        if not estado:\n",
    "            return \"Error: El parámetro 'estado' es necesario en la URL\", 400\n",
    "\n",
    "        # Define el nombre del bucket y la ruta de la carpeta temporal\n",
    "        bucket_name = 'data_lake_pf_henry'\n",
    "        bucket_path = f'Google Maps/reviews-estados/{estado}/'\n",
    "        temporal_folder_path = 'temporal/'\n",
    "\n",
    "        # Crea un cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtiene el bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Lista todos los blobs en la carpeta\n",
    "        blobs = bucket.list_blobs(prefix=bucket_path)\n",
    "\n",
    "        # Inicializa una lista para almacenar los DataFrames de cada archivo\n",
    "        dfs = []\n",
    "\n",
    "        # Itera sobre cada archivo en la carpeta\n",
    "        for blob in blobs:\n",
    "            # Descarga el contenido del blob como un string\n",
    "            json_string = blob.download_as_text()\n",
    "\n",
    "            # Dividir el string por nuevas líneas y cargar cada objeto JSON\n",
    "            json_objects = [json.loads(j) for j in json_string.splitlines()]\n",
    "\n",
    "            # Convierte la lista de objetos JSON en un DataFrame de Pandas\n",
    "            df = pd.DataFrame(json_objects)\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Concatena todos los DataFrames en uno solo\n",
    "        df_combined = pd.concat(dfs)\n",
    "\n",
    "        # Guarda el DataFrame combinado como un archivo CSV en Cloud Storage en la carpeta temporal\n",
    "        csv_file_path = 'gs://{}/{}'.format(bucket_name, temporal_folder_path + 'combined_data.csv')\n",
    "        df_combined.to_csv(csv_file_path, index=False)\n",
    "\n",
    "        # Carga el archivo CSV en BigQuery\n",
    "        to_gbq(df_combined, destination_table='proyecto-final-henry-412703.Datawarehouse_PF.review-Alaska', if_exists='replace')\n",
    "\n",
    "        # Devuelve la respuesta HTTP con el JSON del DataFrame\n",
    "        return \"Carga exitosa!\", 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en la función: {e}\", exc_info=True)\n",
    "        return f\"Error: {e}\", 500\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
