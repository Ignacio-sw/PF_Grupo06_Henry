{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def procesar_archivo_json(archivo_json, tamano_parte):\n",
    "    with open(archivo_json, 'r', encoding=\"utf-8\") as file:\n",
    "        parte_actual = []\n",
    "        num_parte = 1\n",
    "        \n",
    "        for linea in file:\n",
    "            objeto_json = json.loads(linea)\n",
    "            parte_actual.append(objeto_json)\n",
    "            \n",
    "            if len(parte_actual) >= tamano_parte:\n",
    "                # Procesar la parte actual\n",
    "                procesar_y_guardar_parte(parte_actual, num_parte, archivo_json)\n",
    "                parte_actual = []\n",
    "                num_parte += 1\n",
    "                \n",
    "        # Procesar la última parte si quedan datos\n",
    "        if parte_actual:\n",
    "            procesar_y_guardar_parte(parte_actual, num_parte, archivo_json)\n",
    "\n",
    "def procesar_archivo_parquet(archivo_parquet, tamano_parte):\n",
    "    parte_actual = []\n",
    "    num_parte = 1\n",
    "    \n",
    "    # Leer el archivo Parquet en partes\n",
    "    for df_parte in pd.read_parquet(archivo_parquet, chunksize=tamano_parte):\n",
    "        parte_actual.append(df_parte)\n",
    "        \n",
    "        if len(parte_actual) >= tamano_parte:\n",
    "            # Procesar y guardar la parte actual\n",
    "            procesar_y_guardar_parte(parte_actual, num_parte, archivo_parquet)\n",
    "            parte_actual = []\n",
    "            num_parte += 1\n",
    "            \n",
    "    # Procesar y guardar la última parte si quedan datos\n",
    "    if parte_actual:\n",
    "        procesar_y_guardar_parte(parte_actual, num_parte, archivo_parquet)\n",
    "\n",
    "def procesar_y_guardar_parte(parte, num_parte, archivo):\n",
    "    dfs_parte = []\n",
    "    for item in parte:\n",
    "        if isinstance(item, dict):\n",
    "            dfs_parte.append(pd.DataFrame([item]))\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            dfs_parte.append(item)\n",
    "        else:\n",
    "            print(f\"Elemento no válido en la parte {num_parte}: {item}\")\n",
    "    \n",
    "    # Concatenar los DataFrames en un solo DataFrame si hay alguno\n",
    "    if dfs_parte:\n",
    "        df = pd.concat(dfs_parte, ignore_index=True)\n",
    "        \n",
    "        # Determinar el nombre del archivo de salida y la ruta de destino\n",
    "        nombre_archivo = os.path.splitext(os.path.basename(archivo))[0]\n",
    "        ruta_destino = os.path.dirname(archivo)\n",
    "        \n",
    "        # Guardar la parte procesada en un archivo JSON o Parquet según la extensión del archivo original\n",
    "        if archivo.endswith(\".json\"):\n",
    "            df.to_json(os.path.join(ruta_destino, f\"{nombre_archivo}_parte_{num_parte}.json\"), index=False)\n",
    "        elif archivo.endswith(\".parquet\"):\n",
    "            df.to_parquet(os.path.join(ruta_destino, f\"{nombre_archivo}_parte_{num_parte}.parquet\"), index=False)\n",
    "\n",
    "# Directorio donde se encuentran los archivos JSON\n",
    "directorio_origen = r'C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP'\n",
    "tamano_parte = 200000  # Número máximo de registros por parte\n",
    "\n",
    "# Recorrer cada archivo en el directorio de origen\n",
    "for archivo in os.listdir(directorio_origen):\n",
    "    #if archivo.endswith(\".json\"):\n",
    "    #    archivo_json = os.path.join(directorio_origen, archivo)\n",
    "    #    procesar_archivo_json(archivo_json, tamano_parte)\n",
    "    if archivo.endswith(\".parquet\"):\n",
    "        archivo_parquet = os.path.join(directorio_origen, archivo)\n",
    "        procesar_archivo_parquet(archivo_parquet, tamano_parte)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def procesar_archivo_parquet(archivo_parquet, tamano_parte):\n",
    "    df = pd.read_parquet(archivo_parquet)\n",
    "    total_registros = len(df)\n",
    "    num_parte = 1\n",
    "    inicio = 0\n",
    "    \n",
    "    while inicio < total_registros:\n",
    "        fin = min(inicio + tamano_parte, total_registros)\n",
    "        parte_actual = df.iloc[inicio:fin]\n",
    "        procesar_y_guardar_parte(parte_actual, num_parte, archivo_parquet)\n",
    "        num_parte += 1\n",
    "        inicio = fin\n",
    "\n",
    "def procesar_y_guardar_parte(parte, num_parte, archivo):\n",
    "    # Determinar el nombre del archivo de salida y la ruta de destino\n",
    "    nombre_archivo = os.path.splitext(os.path.basename(archivo))[0]\n",
    "    ruta_destino = os.path.dirname(archivo)\n",
    "    \n",
    "    # Guardar la parte procesada en un archivo Parquet\n",
    "    parte.to_json(os.path.join(ruta_destino, f\"{nombre_archivo}_parte_{num_parte}.json\"), index=False)\n",
    "\n",
    "# Directorio donde se encuentran los archivos Parquet\n",
    "directorio_origen = r'C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP'\n",
    "tamano_parte = 100000  # Número máximo de registros por parte\n",
    "\n",
    "# Recorrer cada archivo en el directorio de origen\n",
    "for archivo in os.listdir(directorio_origen):\n",
    "    if archivo.endswith(\".parquet\"):\n",
    "        archivo_parquet = os.path.join(directorio_origen, archivo)\n",
    "        procesar_archivo_parquet(archivo_parquet, tamano_parte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "#from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "    # Crear una instancia del cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Nombre del archivo en el bucket\n",
    "    nombre_archivo = 'df_mascara.csv'\n",
    "\n",
    "    # Nombre del bucket\n",
    "    nombre_bucket = 'yelp_dms'\n",
    "    # Ruta completa en el bucket\n",
    "    ruta_archivo = f'gs://{nombre_bucket}/{nombre_archivo}'\n",
    "    \n",
    "    # Filtrado segun nombre archivo para su respectivo ETL\n",
    "    df_bussines = pd.read_pickle(file_uri)\n",
    "    df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "    print(\"pase la mascara de juan de columnas\")\n",
    "    df_bussines.drop(columns=[\"is_open\",'attributes', 'hours','postal_code'], inplace=True)\n",
    "    categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "\n",
    "    # Eliminar filas con valores NaN en la columna 'categories'\n",
    "    df_bussines = df_bussines.fillna(\"No Data\")\n",
    "    # Filtrar el DataFrame para mantener solo las filas que contienen al menos una de las categorías deseadas\n",
    "    df_bussines = df_bussines[df_bussines['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "    df_mascara= df_bussines[\"business_id\"]\n",
    "    \n",
    "\n",
    "    # Guardar el DataFrame como un archivo CSV\n",
    "    df_mascara.to_csv(nombre_archivo, index=False)\n",
    "    # Obtener el bucket\n",
    "    bucket = client.bucket(nombre_bucket)\n",
    "    # Subir el archivo al bucket\n",
    "    blob = bucket.blob(nombre_archivo)\n",
    "    blob.upload_from_filename(nombre_archivo)\n",
    "    print(f'DataFrame guardado en {ruta_archivo}')\n",
    "    return df_bussines\n",
    "\n",
    "def archivo_df_tip(file_uri):\n",
    "    df_tip = pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "        if df_tip[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return df_tip\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001 = pd.read_json(file_uri, lines=True)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "        if df_user001[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    print(\"completado etl archivo parquet\")\n",
    "    return df_user001\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    df_review = pd.read_json(file_uri, lines=True)\n",
    "    for columna in df_review.columns:\n",
    "        #if df_review[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "        #    df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "        if df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_review[columna] = df_review[columna].fillna(0)\n",
    "    df_mascara=pd.read_csv(\"gs://yelp_dms/df_mascara.csv\")\n",
    "    print(\"carga de mascara completada\")\n",
    "    df_review = pd.merge(df_review, df_mascara, on='business_id', how='inner')\n",
    "    print(\"filtracion completa\")\n",
    "    return df_review\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "    data = cloud_event.data\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "    bucket_name = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    print(f'Bucket: {bucket_name}')\n",
    "    print(f'File name: {name}')\n",
    "    # Create URI\n",
    "    file_uri = f'gs://{bucket_name}/{name}'\n",
    "    print(f'URI: {file_uri}')\n",
    "    # Obtener el objeto del bucket\n",
    "    bucket = cloud_storage.bucket(bucket_name)\n",
    "    # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "    blob = bucket.blob(name)\n",
    "    # Attempt to create DataFrame with URI\n",
    "    # Attempt to process DataFrame\n",
    "    print('Transformando dataframe...')\n",
    "    try:\n",
    "        if \"business\" in file_uri:\n",
    "            df_procesado = archivo_df_business(file_uri)\n",
    "            table_name = \"yelp-sites\"\n",
    "            print(\"dataframe cargado\") \n",
    "        if \"tip\" in file_uri:\n",
    "            df_procesado = archivo_df_tip(file_uri)\n",
    "            table_name = \"yelp-user-tips\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"user-001\" in file_uri:\n",
    "            df_procesado = archivo_df_user(file_uri)\n",
    "            table_name = \"yelp-user-info\"\n",
    "            print(\"dataframe USER-001 cargado\")\n",
    "        if \"review-002\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "            print(\"dataframe REVIEW-002 cargado\")\n",
    "        print('DataFrame Procesado')\n",
    "        print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    except Exception as e:\n",
    "        # Cuando no procesa la información\n",
    "        print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "        return None\n",
    "    # Insert to table\n",
    "    table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        create_disposition='CREATE_IF_NEEDED',\n",
    "        write_disposition='WRITE_APPEND'\n",
    "        # source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "    print('Job configuration created.')\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_dataframe(\n",
    "        df_procesado,\n",
    "        table_id,\n",
    "        location='us-central1',\n",
    "        job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    print('Job finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002_parte_1.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id      object\n",
      "user_id        object\n",
      "business_id    object\n",
      "stars          object\n",
      "useful         object\n",
      "funny          object\n",
      "cool           object\n",
      "text           object\n",
      "date           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_aux['business_id'] = df_aux['business_id'].astype(str)\n",
    "\n",
    "\n",
    "# Verificar el cambio\n",
    "print(df_aux.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mascara=pd.read_csv(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\df_mascara(1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux2=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002_parte_2.json\", lines=True)\n",
    "df_aux3=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002_parte_3.json\", lines=True)\n",
    "df_aux4=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002_parte_4.json\", lines=True)\n",
    "df_aux2['business_id'] = df_aux2['business_id'].astype(str)\n",
    "df_aux2['business_id'] = df_aux2['business_id'].str.strip()\n",
    "df_aux3['business_id'] = df_aux3['business_id'].astype(str)\n",
    "df_aux3['business_id'] = df_aux3['business_id'].str.strip()\n",
    "df_aux4['business_id'] = df_aux4['business_id'].astype(str)\n",
    "df_aux4['business_id'] = df_aux4['business_id'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = pd.merge(df_aux4, df_mascara, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002.json\",lines=True, chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parte 1 guardada como parte_1.json\n",
      "Parte 2 guardada como parte_2.json\n",
      "Parte 3 guardada como parte_3.json\n",
      "Parte 4 guardada como parte_4.json\n",
      "Parte 5 guardada como parte_5.json\n",
      "Parte 6 guardada como parte_6.json\n",
      "Parte 7 guardada como parte_7.json\n",
      "Parte 8 guardada como parte_8.json\n",
      "Parte 9 guardada como parte_9.json\n",
      "Parte 10 guardada como parte_10.json\n",
      "Parte 11 guardada como parte_11.json\n",
      "Parte 12 guardada como parte_12.json\n",
      "Parte 13 guardada como parte_13.json\n",
      "Parte 14 guardada como parte_14.json\n",
      "Parte 15 guardada como parte_15.json\n",
      "Parte 16 guardada como parte_16.json\n",
      "Parte 17 guardada como parte_17.json\n",
      "Parte 18 guardada como parte_18.json\n",
      "Parte 19 guardada como parte_19.json\n",
      "Parte 20 guardada como parte_20.json\n",
      "Parte 21 guardada como parte_21.json\n",
      "Parte 22 guardada como parte_22.json\n",
      "Parte 23 guardada como parte_23.json\n",
      "Parte 24 guardada como parte_24.json\n",
      "Parte 25 guardada como parte_25.json\n",
      "Parte 26 guardada como parte_26.json\n",
      "Parte 27 guardada como parte_27.json\n",
      "Parte 28 guardada como parte_28.json\n",
      "Parte 29 guardada como parte_29.json\n",
      "Parte 30 guardada como parte_30.json\n",
      "Parte 31 guardada como parte_31.json\n",
      "Parte 32 guardada como parte_32.json\n",
      "Parte 33 guardada como parte_33.json\n",
      "Parte 34 guardada como parte_34.json\n",
      "Parte 35 guardada como parte_35.json\n",
      "Parte 36 guardada como parte_36.json\n",
      "Parte 37 guardada como parte_37.json\n",
      "Parte 38 guardada como parte_38.json\n",
      "Parte 39 guardada como parte_39.json\n",
      "Parte 40 guardada como parte_40.json\n",
      "Parte 41 guardada como parte_41.json\n",
      "Parte 42 guardada como parte_42.json\n",
      "Parte 43 guardada como parte_43.json\n",
      "Parte 44 guardada como parte_44.json\n",
      "Parte 45 guardada como parte_45.json\n",
      "Parte 46 guardada como parte_46.json\n",
      "Parte 47 guardada como parte_47.json\n",
      "Parte 48 guardada como parte_48.json\n",
      "Parte 49 guardada como parte_49.json\n",
      "Parte 50 guardada como parte_50.json\n",
      "Parte 51 guardada como parte_51.json\n",
      "Parte 52 guardada como parte_52.json\n",
      "Parte 53 guardada como parte_53.json\n",
      "Parte 54 guardada como parte_54.json\n",
      "Parte 55 guardada como parte_55.json\n",
      "Parte 56 guardada como parte_56.json\n",
      "Parte 57 guardada como parte_57.json\n",
      "Parte 58 guardada como parte_58.json\n",
      "Parte 59 guardada como parte_59.json\n",
      "Parte 60 guardada como parte_60.json\n",
      "Parte 61 guardada como parte_61.json\n",
      "Parte 62 guardada como parte_62.json\n",
      "Parte 63 guardada como parte_63.json\n",
      "Parte 64 guardada como parte_64.json\n",
      "Parte 65 guardada como parte_65.json\n",
      "Parte 66 guardada como parte_66.json\n",
      "Parte 67 guardada como parte_67.json\n",
      "Parte 68 guardada como parte_68.json\n",
      "Parte 69 guardada como parte_69.json\n",
      "Parte 70 guardada como parte_70.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "archivo_json = r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\review-002.json\"\n",
    "chunksize = 100000  # Tamaño de cada parte\n",
    "output_directory = r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\partes\"  # Directorio de salida\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Leer el archivo JSON en partes más pequeñas\n",
    "partes = pd.read_json(archivo_json, lines=True, chunksize=chunksize)\n",
    "\n",
    "# Procesar cada parte por separado\n",
    "for i, parte in enumerate(partes):\n",
    "    # Determinar el nombre del archivo de salida\n",
    "    nombre_archivo = f\"parte_{i+1}.json\"\n",
    "    ruta_archivo = os.path.join(output_directory, nombre_archivo)\n",
    "    \n",
    "    # Guardar la parte como archivo JSON\n",
    "    parte.to_json(ruta_archivo, orient=\"records\", lines=True)\n",
    "    \n",
    "    print(f\"Parte {i+1} guardada como {nombre_archivo}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parte=pd.read_json(r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\partes\\parte_1.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "particion user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parte 1 guardada como user_1.json\n",
      "Parte 2 guardada como user_2.json\n",
      "Parte 3 guardada como user_3.json\n",
      "Parte 4 guardada como user_4.json\n",
      "Parte 5 guardada como user_5.json\n",
      "Parte 6 guardada como user_6.json\n",
      "Parte 7 guardada como user_7.json\n",
      "Parte 8 guardada como user_8.json\n",
      "Parte 9 guardada como user_9.json\n",
      "Parte 10 guardada como user_10.json\n",
      "Parte 11 guardada como user_11.json\n",
      "Parte 12 guardada como user_12.json\n",
      "Parte 13 guardada como user_13.json\n",
      "Parte 14 guardada como user_14.json\n",
      "Parte 15 guardada como user_15.json\n",
      "Parte 16 guardada como user_16.json\n",
      "Parte 17 guardada como user_17.json\n",
      "Parte 18 guardada como user_18.json\n",
      "Parte 19 guardada como user_19.json\n",
      "Parte 20 guardada como user_20.json\n",
      "Parte 21 guardada como user_21.json\n",
      "Parte 22 guardada como user_22.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "archivo_parquet = r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\\user-001.parquet\"\n",
    "chunksize = 100000  # Tamaño de cada parte\n",
    "output_directory = r\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataLake\\YELP\"  # Directorio de salida\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Leer el archivo Parquet completo\n",
    "df = pd.read_parquet(archivo_parquet)\n",
    "\n",
    "# Dividir el DataFrame en partes más pequeñas\n",
    "num_partes = len(df) // chunksize + 1\n",
    "for i, start_idx in enumerate(range(0, len(df), chunksize)):\n",
    "    # Determinar el nombre del archivo de salida\n",
    "    nombre_archivo = f\"user_{i+1}.json\"\n",
    "    ruta_archivo = os.path.join(output_directory, nombre_archivo)\n",
    "    \n",
    "    # Obtener la parte actual del DataFrame\n",
    "    parte = df.iloc[start_idx:start_idx+chunksize]\n",
    "    \n",
    "    # Guardar la parte como archivo JSON\n",
    "    parte.to_json(ruta_archivo, orient=\"records\", lines=True)\n",
    "    \n",
    "    print(f\"Parte {i+1} guardada como {nombre_archivo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Importar modulo processing\n",
    "#from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "    # Crear una instancia del cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Nombre del archivo en el bucket\n",
    "    nombre_archivo = 'df_mascara.csv'\n",
    "\n",
    "    # Nombre del bucket\n",
    "    nombre_bucket = 'yelp_dms'\n",
    "    # Ruta completa en el bucket\n",
    "    ruta_archivo = f'gs://{nombre_bucket}/{nombre_archivo}'\n",
    "    \n",
    "    # Filtrado segun nombre archivo para su respectivo ETL\n",
    "    df_bussines = pd.read_pickle(file_uri)\n",
    "    df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "    print(\"pase la mascara de juan de columnas\")\n",
    "    df_bussines.drop(columns=[\"is_open\",'attributes', 'hours','postal_code'], inplace=True)\n",
    "    categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "\n",
    "    # Eliminar filas con valores NaN en la columna 'categories'\n",
    "    df_bussines = df_bussines.fillna(\"No Data\")\n",
    "    # Filtrar el DataFrame para mantener solo las filas que contienen al menos una de las categorías deseadas\n",
    "    df_bussines = df_bussines[df_bussines['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "    df_mascara= df_bussines[\"business_id\"]\n",
    "    \n",
    "\n",
    "    # Guardar el DataFrame como un archivo CSV\n",
    "    df_mascara.to_csv(nombre_archivo, index=False)\n",
    "    # Obtener el bucket\n",
    "    bucket = client.bucket(nombre_bucket)\n",
    "    # Subir el archivo al bucket\n",
    "    blob = bucket.blob(nombre_archivo)\n",
    "    blob.upload_from_filename(nombre_archivo)\n",
    "    print(f'DataFrame guardado en {ruta_archivo}')\n",
    "    return df_bussines\n",
    "\n",
    "def archivo_df_tip(file_uri):\n",
    "    df_tip = pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "        if df_tip[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return df_tip\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001 = pd.read_json(file_uri, lines=True)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "        if df_user001[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    print(\"completado etl archivo parquet\")\n",
    "    return df_user001\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    df_review = pd.read_json(file_uri, lines=True)\n",
    "    for columna in df_review.columns:\n",
    "        #if df_review[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "        #    df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "        if df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_review[columna] = df_review[columna].fillna(0)\n",
    "    df_mascara=pd.read_csv(\"gs://yelp_dms/df_mascara.csv\")\n",
    "    print(\"carga de mascara completada\")\n",
    "    df_review['business_id'] = df_review['business_id'].astype(str)\n",
    "    df_mascara['business_id'] = df_mascara['business_id'].astype(str)\n",
    "\n",
    "    df_review = pd.merge(df_review, df_mascara, on='business_id', how='inner')\n",
    "    print(\"filtracion completa en archivo REVIEW\")\n",
    "    return df_review\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "    data = cloud_event.data\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "    bucket_name = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    print(f'Bucket: {bucket_name}')\n",
    "    print(f'File name: {name}')\n",
    "    # Create URI\n",
    "    file_uri = f'gs://{bucket_name}/{name}'\n",
    "    print(f'URI: {file_uri}')\n",
    "    # Obtener el objeto del bucket\n",
    "    bucket = cloud_storage.bucket(bucket_name)\n",
    "    # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "    blob = bucket.blob(name)\n",
    "    # Attempt to create DataFrame with URI\n",
    "    # Attempt to process DataFrame\n",
    "    print('Transformando dataframe...')\n",
    "    try:\n",
    "        if \"business\" in file_uri:\n",
    "            df_procesado = archivo_df_business(file_uri)\n",
    "            table_name = \"yelp-sites\"\n",
    "            print(\"dataframe cargado\") \n",
    "        if \"tip\" in file_uri:\n",
    "            df_procesado = archivo_df_tip(file_uri)\n",
    "            table_name = \"yelp-user-tips\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"user-001\" in file_uri:\n",
    "            df_procesado = archivo_df_user(file_uri)\n",
    "            table_name = \"yelp-user-info\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"review-002\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "            print(\"dataframe cargado\")\n",
    "        print('DataFrame Procesado')\n",
    "        print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    except Exception as e:\n",
    "        # Cuando no procesa la información\n",
    "        print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "        return None\n",
    "    # Insert to table\n",
    "    table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        create_disposition='CREATE_IF_NEEDED',\n",
    "        write_disposition='WRITE_APPEND'\n",
    "        # source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "    print('Job configuration created.')\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_dataframe(\n",
    "        df_procesado,\n",
    "        table_id,\n",
    "        location='us-central1',\n",
    "        job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    print('Job finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(R\"C:\\Users\\Usuario\\Desktop\\GitHub\\DataSets\\DataWarehouse\\business.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(file_uri)\n",
    "df=df.loc[:,~df.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "from tqdm import tqdm\n",
    "\n",
    "def obtener_estado_y_ciudad(latitudes, longitudes):\n",
    "    total_registros = len(latitudes)\n",
    "    resultados = []\n",
    "    \n",
    "    # Inicializar barra de progreso\n",
    "    with tqdm(total=total_registros, desc=\"Procesando\") as pbar:\n",
    "        for lat, lon in zip(latitudes, longitudes):\n",
    "            coordinates = (lat, lon)\n",
    "            result = rg.search(coordinates)  # Realiza la búsqueda inversa\n",
    "            estado = result[0]['admin1']  # Obtiene el estado de la respuesta\n",
    "            ciudad = result[0]['name']  # Obtiene el nombre de la ciudad de la respuesta\n",
    "            resultados.append((estado, ciudad))\n",
    "            pbar.update(1)  # Incrementar la barra de progreso\n",
    "            pbar.set_postfix({\"Completado\": f\"{pbar.n}/{pbar.total}\"})  # Mostrar porcentaje completado\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Usar la función con las columnas de latitud y longitud del DataFrame\n",
    "resultados = obtener_estado_y_ciudad(df['latitude'], df['longitude'])\n",
    "\n",
    "# Crear columnas \"state\" y \"city\" con los resultados\n",
    "df[['state', 'city']] = pd.DataFrame(resultados, columns=['state', 'city'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo para corregir nombre de estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 150346/150346 (100.00%) - Tiempo restante: 0h 0m 0s\n",
      "Tiempo total de ejecución: 27.20 segundos\n",
      "                   business_id                      name  \\\n",
      "0       Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
      "1       mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
      "2       tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
      "3       MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
      "4       mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
      "...                        ...                       ...   \n",
      "150341  IUQopTMmYQG-qRtBk-8QnA              Binh's Nails   \n",
      "150342  c8GjPIOTGVmIemT7j5_SyQ      Wild Birds Unlimited   \n",
      "150343  _QAMST-NrQobXduilWEqSw         Claire's Boutique   \n",
      "150344  mtGm22y5c2UHNXDFAjaPNw  Cyclery & Fitness Center   \n",
      "150345  jV_XOycEzSlTx-65W906pg                   Sic Ink   \n",
      "\n",
      "                                address           city          state  \\\n",
      "0                1616 Chapala St, Ste 2  Santa Barbara  No encontrado   \n",
      "1       87 Grasso Plaza Shopping Center         Affton  No encontrado   \n",
      "2                  5255 E Broadway Blvd         Tucson  No encontrado   \n",
      "3                           935 Race St   Philadelphia  No encontrado   \n",
      "4                         101 Walnut St     Green Lane  No encontrado   \n",
      "...                                 ...            ...            ...   \n",
      "150341                3388 Gateway Blvd       Edmonton  No encontrado   \n",
      "150342               2813 Bransford Ave      Nashville  No encontrado   \n",
      "150343           6020 E 82nd St, Ste 46   Indianapolis  No encontrado   \n",
      "150344                     2472 Troy Rd   Edwardsville  No encontrado   \n",
      "150345            238 Apollo Beach Blvd   Apollo beach  No encontrado   \n",
      "\n",
      "         latitude   longitude  stars  review_count  is_open  \\\n",
      "0       34.426679 -119.711197    5.0             7        0   \n",
      "1       38.551126  -90.335695    3.0            15        1   \n",
      "2       32.223236 -110.880452    3.5            22        0   \n",
      "3       39.955505  -75.155564    4.0            80        1   \n",
      "4       40.338183  -75.471659    4.5            13        1   \n",
      "...           ...         ...    ...           ...      ...   \n",
      "150341  53.468419 -113.492054    3.0            13        1   \n",
      "150342  36.115118  -86.766925    4.0             5        1   \n",
      "150343  39.908707  -86.065088    3.5             8        1   \n",
      "150344  38.782351  -89.950558    4.0            24        1   \n",
      "150345  27.771002  -82.394910    4.5             9        1   \n",
      "\n",
      "                                               categories  \n",
      "0       Doctors, Traditional Chinese Medicine, Naturop...  \n",
      "1       Shipping Centers, Local Services, Notaries, Ma...  \n",
      "2       Department Stores, Shopping, Fashion, Home & G...  \n",
      "3       Restaurants, Food, Bubble Tea, Coffee & Tea, B...  \n",
      "4                               Brewpubs, Breweries, Food  \n",
      "...                                                   ...  \n",
      "150341                         Nail Salons, Beauty & Spas  \n",
      "150342  Pets, Nurseries & Gardening, Pet Stores, Hobby...  \n",
      "150343  Shopping, Jewelry, Piercing, Toy Stores, Beaut...  \n",
      "150344  Fitness/Exercise Equipment, Eyewear & Optician...  \n",
      "150345  Beauty & Spas, Permanent Makeup, Piercing, Tattoo  \n",
      "\n",
      "[150346 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import reverse_geocoder as rg\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def obtener_estado(ciudad):\n",
    "    try:\n",
    "        result = rg.search((ciudad, 0))  # Realiza la búsqueda inversa\n",
    "        print(result)  # Imprimir el resultado de la búsqueda para depuración\n",
    "        estado = result[0]['admin1']  # Obtiene el estado de la respuesta\n",
    "        return estado\n",
    "    except:\n",
    "        return \"No encontrado\"\n",
    "\n",
    "# Suponiendo que 'df' es tu DataFrame\n",
    "inicio = time.time()  # Tiempo inicial\n",
    "total_ciudades = len(df['city'])\n",
    "for i, ciudad in enumerate(df['city'], 1):\n",
    "    # Calcular el tiempo transcurrido y estimar el tiempo restante\n",
    "    tiempo_transcurrido = time.time() - inicio\n",
    "    tiempo_restante = (tiempo_transcurrido / i) * (total_ciudades - i)\n",
    "    horas_restantes = int(tiempo_restante // 3600)\n",
    "    minutos_restantes = int((tiempo_restante % 3600) // 60)\n",
    "    segundos_restantes = int(tiempo_restante % 60)\n",
    "    \n",
    "    # Mostrar el progreso y el tiempo restante\n",
    "    print(f\"Procesando: {i}/{total_ciudades} ({100*i/total_ciudades:.2f}%) - Tiempo restante: {horas_restantes}h {minutos_restantes}m {segundos_restantes}s\", end=\"\\r\")\n",
    "    \n",
    "    # Aplicar la función obtener_estado\n",
    "    df.at[i-1, 'state'] = obtener_estado(ciudad)\n",
    "\n",
    "fin = time.time()  # Tiempo final\n",
    "\n",
    "# Calcular el tiempo total de ejecución\n",
    "tiempo_total = fin - inicio\n",
    "\n",
    "# Imprimir el tiempo total de ejecución\n",
    "print(f\"\\nTiempo total de ejecución: {tiempo_total:.2f} segundos\")\n",
    "\n",
    "# Imprimir el DataFrame con las nuevas columnas\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47179</th>\n",
       "      <td>jywqOvyPFJ4NT3KdcaNiGg</td>\n",
       "      <td>Dennis M Lox, MD - Sports and Regenerative Med...</td>\n",
       "      <td>2030 Drew St</td>\n",
       "      <td>Clearwater</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>27.968249</td>\n",
       "      <td>-82.752792</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Medical, Doctors, Pain Management, Ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73073</th>\n",
       "      <td>edBkcpIZlX6tkXUc0A2e0Q</td>\n",
       "      <td>Courtyard Indianapolis at the Capitol</td>\n",
       "      <td>320 North Senate Avenue</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.773010</td>\n",
       "      <td>-86.164672</td>\n",
       "      <td>3.5</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotels, Hotels &amp; Travel, Event Planning &amp; Serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7262</th>\n",
       "      <td>LCYE3aQGQf74Z13U2g81Lg</td>\n",
       "      <td>Plymouth Auto &amp; Tire Center</td>\n",
       "      <td>2014 Butler Pike</td>\n",
       "      <td>Plymouth Meeting</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>40.097040</td>\n",
       "      <td>-75.284217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>Automotive, Auto Repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>KVQQ0C3t0RJAUKeQK4kzkw</td>\n",
       "      <td>American Dental Solutions</td>\n",
       "      <td>545 W Main St</td>\n",
       "      <td>Trappe</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>40.200083</td>\n",
       "      <td>-75.478121</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Medical, Dentists, Oral Surgeons, Gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35238</th>\n",
       "      <td>cino46ny3O8RbGYZniVn5g</td>\n",
       "      <td>Infinite Endodontics</td>\n",
       "      <td>1765 Springdale Rd, Ste C1</td>\n",
       "      <td>Cherry Hill</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.899651</td>\n",
       "      <td>-74.966712</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Endodontists, General Dentistry, Health &amp; Medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57977</th>\n",
       "      <td>LrZRyMmDbv065ZU4Czfgaw</td>\n",
       "      <td>Enterprise Rent-A-Car</td>\n",
       "      <td>706 Garrett Rd</td>\n",
       "      <td>Upper Darby</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.958017</td>\n",
       "      <td>-75.266992</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotels &amp; Travel, Car Rental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>4JzJG4KWRDFDROb8CjmFxQ</td>\n",
       "      <td>Motor Sheep</td>\n",
       "      <td>5000 Smithridge Dr, Ste A7, Smithridge Shoppin...</td>\n",
       "      <td>Reno</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.478213</td>\n",
       "      <td>-119.789037</td>\n",
       "      <td>4.5</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Auto Upholstery, Furniture Reupholstery, Shopp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12057</th>\n",
       "      <td>s0MmNLweVTJsfmbGeqasag</td>\n",
       "      <td>Auto Tags &amp; Insurance</td>\n",
       "      <td>702 MacDade Blvd</td>\n",
       "      <td>Collingdale</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.911974</td>\n",
       "      <td>-75.276561</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Financial Services, Auto Insurance, Motorcycle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138008</th>\n",
       "      <td>yX0P0_JV1imBE46I1ZMEaA</td>\n",
       "      <td>Panera Bread</td>\n",
       "      <td>2534 Powell Ave, Ste 120</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>36.106011</td>\n",
       "      <td>-86.763501</td>\n",
       "      <td>2.5</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>Salad, Bagels, Soup, Breakfast &amp; Brunch, Resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24929</th>\n",
       "      <td>gQr86APLp8pmRVJlNIOvbQ</td>\n",
       "      <td>Waterway Carwash</td>\n",
       "      <td>10559 Old Olive St</td>\n",
       "      <td>Creve Coeur</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>38.674994</td>\n",
       "      <td>-90.412997</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>Car Wash, Food, Gas Stations, Automotive, Conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125043</th>\n",
       "      <td>m0p0IuN3s8GG1jrZOPQssA</td>\n",
       "      <td>Mister Car Wash</td>\n",
       "      <td>4024 Nolensville Pike</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>36.083936</td>\n",
       "      <td>-86.727197</td>\n",
       "      <td>2.5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Automotive, Car Wash, Auto Detailing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7912</th>\n",
       "      <td>8qCbeHopky8xpGndJCcr7w</td>\n",
       "      <td>eegee's</td>\n",
       "      <td>502 W Ajo Way, Ste 100</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>32.178532</td>\n",
       "      <td>-110.976302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Sandwiches, Juice Bars &amp; Smoothies, Restaurant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50572</th>\n",
       "      <td>soOsa3dqKjECZnxCPTdWjw</td>\n",
       "      <td>Winter Wonderland @ Tilles Park</td>\n",
       "      <td>9551 Litzinger Rd</td>\n",
       "      <td>Saint Louis</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>38.619600</td>\n",
       "      <td>-90.364286</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>Arts &amp; Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13981</th>\n",
       "      <td>OirY4tBv5wFsOwTggMhAKQ</td>\n",
       "      <td>Westlake Animal Inn Pet Resort &amp; Grooming</td>\n",
       "      <td>39564 Us Highway 19 N</td>\n",
       "      <td>Tarpon Springs</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>28.131826</td>\n",
       "      <td>-82.741162</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Pet Sitting, Pets, Pet Services, Pet Groomers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>xAwnIKLl9zPA6fAe_1AbqA</td>\n",
       "      <td>VRC</td>\n",
       "      <td>340 Lancaster Ave</td>\n",
       "      <td>Malvern</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>40.037813</td>\n",
       "      <td>-75.551597</td>\n",
       "      <td>3.5</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>Personal Shopping, Shopping, Pet Services, Per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38281</th>\n",
       "      <td>JWetIu1H1Lhfpx23h3aVHg</td>\n",
       "      <td>Walgreens</td>\n",
       "      <td>4545 W Esplanade Ave</td>\n",
       "      <td>Metairie</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>30.016889</td>\n",
       "      <td>-90.187576</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Shopping, Drugstores, Beauty &amp; Spas, Convenien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83610</th>\n",
       "      <td>2vIw_EuAcham4RFKh5ctIw</td>\n",
       "      <td>Bath &amp; Body Works</td>\n",
       "      <td>1625 Chestnut Street</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>39.951583</td>\n",
       "      <td>-75.168006</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Shopping, Home &amp; Garden, Cosmetics &amp; Beauty Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>SLmGshQmUoMPMlwbMuLWLQ</td>\n",
       "      <td>Avola Kitchen + Bar</td>\n",
       "      <td>625 N Morehall Rd</td>\n",
       "      <td>Malvern</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>40.068025</td>\n",
       "      <td>-75.536131</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>Sicilian, Tapas Bars, Pizza, Italian, Nightlif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45566</th>\n",
       "      <td>y1htpGz1bd7PF7IUKZ42HA</td>\n",
       "      <td>Applebee's Grill + Bar</td>\n",
       "      <td>4300 Green Mount Crossing Dr</td>\n",
       "      <td>Shiloh</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>38.571822</td>\n",
       "      <td>-89.928524</td>\n",
       "      <td>2.5</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Steakhouses, American (Traditional), Sports Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88719</th>\n",
       "      <td>Av2lr9DThZFPXO20TnlHdQ</td>\n",
       "      <td>Rao Plastic &amp; Hand Surgery</td>\n",
       "      <td>5170 E Glenn St, Ste 100</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>No encontrado</td>\n",
       "      <td>32.257665</td>\n",
       "      <td>-110.882164</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Hair Removal, Medical Spas, Skin Care, Cosmeti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   business_id  \\\n",
       "47179   jywqOvyPFJ4NT3KdcaNiGg   \n",
       "73073   edBkcpIZlX6tkXUc0A2e0Q   \n",
       "7262    LCYE3aQGQf74Z13U2g81Lg   \n",
       "7764    KVQQ0C3t0RJAUKeQK4kzkw   \n",
       "35238   cino46ny3O8RbGYZniVn5g   \n",
       "57977   LrZRyMmDbv065ZU4Czfgaw   \n",
       "23556   4JzJG4KWRDFDROb8CjmFxQ   \n",
       "12057   s0MmNLweVTJsfmbGeqasag   \n",
       "138008  yX0P0_JV1imBE46I1ZMEaA   \n",
       "24929   gQr86APLp8pmRVJlNIOvbQ   \n",
       "125043  m0p0IuN3s8GG1jrZOPQssA   \n",
       "7912    8qCbeHopky8xpGndJCcr7w   \n",
       "50572   soOsa3dqKjECZnxCPTdWjw   \n",
       "13981   OirY4tBv5wFsOwTggMhAKQ   \n",
       "27005   xAwnIKLl9zPA6fAe_1AbqA   \n",
       "38281   JWetIu1H1Lhfpx23h3aVHg   \n",
       "83610   2vIw_EuAcham4RFKh5ctIw   \n",
       "10997   SLmGshQmUoMPMlwbMuLWLQ   \n",
       "45566   y1htpGz1bd7PF7IUKZ42HA   \n",
       "88719   Av2lr9DThZFPXO20TnlHdQ   \n",
       "\n",
       "                                                     name  \\\n",
       "47179   Dennis M Lox, MD - Sports and Regenerative Med...   \n",
       "73073               Courtyard Indianapolis at the Capitol   \n",
       "7262                          Plymouth Auto & Tire Center   \n",
       "7764                            American Dental Solutions   \n",
       "35238                                Infinite Endodontics   \n",
       "57977                               Enterprise Rent-A-Car   \n",
       "23556                                         Motor Sheep   \n",
       "12057                               Auto Tags & Insurance   \n",
       "138008                                       Panera Bread   \n",
       "24929                                    Waterway Carwash   \n",
       "125043                                    Mister Car Wash   \n",
       "7912                                              eegee's   \n",
       "50572                     Winter Wonderland @ Tilles Park   \n",
       "13981           Westlake Animal Inn Pet Resort & Grooming   \n",
       "27005                                                 VRC   \n",
       "38281                                           Walgreens   \n",
       "83610                                   Bath & Body Works   \n",
       "10997                                 Avola Kitchen + Bar   \n",
       "45566                              Applebee's Grill + Bar   \n",
       "88719                          Rao Plastic & Hand Surgery   \n",
       "\n",
       "                                                  address              city  \\\n",
       "47179                                        2030 Drew St        Clearwater   \n",
       "73073                             320 North Senate Avenue      Indianapolis   \n",
       "7262                                     2014 Butler Pike  Plymouth Meeting   \n",
       "7764                                        545 W Main St            Trappe   \n",
       "35238                          1765 Springdale Rd, Ste C1       Cherry Hill   \n",
       "57977                                      706 Garrett Rd       Upper Darby   \n",
       "23556   5000 Smithridge Dr, Ste A7, Smithridge Shoppin...              Reno   \n",
       "12057                                    702 MacDade Blvd       Collingdale   \n",
       "138008                           2534 Powell Ave, Ste 120         Nashville   \n",
       "24929                                  10559 Old Olive St       Creve Coeur   \n",
       "125043                              4024 Nolensville Pike         Nashville   \n",
       "7912                               502 W Ajo Way, Ste 100            Tucson   \n",
       "50572                                   9551 Litzinger Rd       Saint Louis   \n",
       "13981                               39564 Us Highway 19 N    Tarpon Springs   \n",
       "27005                                   340 Lancaster Ave           Malvern   \n",
       "38281                                4545 W Esplanade Ave          Metairie   \n",
       "83610                                1625 Chestnut Street      Philadelphia   \n",
       "10997                                   625 N Morehall Rd           Malvern   \n",
       "45566                        4300 Green Mount Crossing Dr            Shiloh   \n",
       "88719                            5170 E Glenn St, Ste 100            Tucson   \n",
       "\n",
       "                state   latitude   longitude  stars  review_count  is_open  \\\n",
       "47179   No encontrado  27.968249  -82.752792    4.0             7        1   \n",
       "73073   No encontrado  39.773010  -86.164672    3.5            38        1   \n",
       "7262    No encontrado  40.097040  -75.284217    2.0            71        1   \n",
       "7764    No encontrado  40.200083  -75.478121    3.5             7        1   \n",
       "35238   No encontrado  39.899651  -74.966712    3.0            14        0   \n",
       "57977   No encontrado  39.958017  -75.266992    2.5            11        1   \n",
       "23556   No encontrado  39.478213 -119.789037    4.5            18        1   \n",
       "12057   No encontrado  39.911974  -75.276561    5.0             6        1   \n",
       "138008  No encontrado  36.106011  -86.763501    2.5            77        1   \n",
       "24929   No encontrado  38.674994  -90.412997    3.0            32        1   \n",
       "125043  No encontrado  36.083936  -86.727197    2.5            16        1   \n",
       "7912    No encontrado  32.178532 -110.976302    3.0            25        1   \n",
       "50572   No encontrado  38.619600  -90.364286    4.0            20        1   \n",
       "13981   No encontrado  28.131826  -82.741162    4.5             6        1   \n",
       "27005   No encontrado  40.037813  -75.551597    3.5            75        1   \n",
       "38281   No encontrado  30.016889  -90.187576    2.5            11        1   \n",
       "83610   No encontrado  39.951583  -75.168006    3.0             7        1   \n",
       "10997   No encontrado  40.068025  -75.536131    4.0            86        1   \n",
       "45566   No encontrado  38.571822  -89.928524    2.5            41        1   \n",
       "88719   No encontrado  32.257665 -110.882164    4.5             5        1   \n",
       "\n",
       "                                               categories  \n",
       "47179   Health & Medical, Doctors, Pain Management, Ph...  \n",
       "73073   Hotels, Hotels & Travel, Event Planning & Serv...  \n",
       "7262                              Automotive, Auto Repair  \n",
       "7764    Health & Medical, Dentists, Oral Surgeons, Gen...  \n",
       "35238   Endodontists, General Dentistry, Health & Medi...  \n",
       "57977                         Hotels & Travel, Car Rental  \n",
       "23556   Auto Upholstery, Furniture Reupholstery, Shopp...  \n",
       "12057   Financial Services, Auto Insurance, Motorcycle...  \n",
       "138008  Salad, Bagels, Soup, Breakfast & Brunch, Resta...  \n",
       "24929   Car Wash, Food, Gas Stations, Automotive, Conv...  \n",
       "125043               Automotive, Car Wash, Auto Detailing  \n",
       "7912    Sandwiches, Juice Bars & Smoothies, Restaurant...  \n",
       "50572                                Arts & Entertainment  \n",
       "13981   Pet Sitting, Pets, Pet Services, Pet Groomers,...  \n",
       "27005   Personal Shopping, Shopping, Pet Services, Per...  \n",
       "38281   Shopping, Drugstores, Beauty & Spas, Convenien...  \n",
       "83610   Shopping, Home & Garden, Cosmetics & Beauty Su...  \n",
       "10997   Sicilian, Tapas Bars, Pizza, Italian, Nightlif...  \n",
       "45566   Steakhouses, American (Traditional), Sports Ba...  \n",
       "88719   Hair Removal, Medical Spas, Skin Care, Cosmeti...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codigo que estoy probando con geo en cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_framework\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Importar modulo processing\n",
    "#from processing import procesar, hash_category\n",
    "\n",
    "# Instatiate clients\n",
    "cloud_storage = storage.Client()\n",
    "bq = bigquery.Client()\n",
    "\n",
    "project_id = 'dms-pfh'\n",
    "\n",
    "\n",
    "def archivo_df_business(file_uri):\n",
    "    # Crear una instancia del cliente de Google Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Nombre del archivo en el bucket\n",
    "    nombre_archivo = 'df_mascara.csv'\n",
    "\n",
    "    # Nombre del bucket\n",
    "    nombre_bucket = 'yelp_dms'\n",
    "    # Ruta completa en el bucket\n",
    "    ruta_archivo = f'gs://{nombre_bucket}/{nombre_archivo}'\n",
    "    \n",
    "    # Filtrado segun nombre archivo para su respectivo ETL\n",
    "    df_bussines = pd.read_pickle(file_uri)\n",
    "    df_bussines =df_bussines.loc[:,~df_bussines.columns.duplicated()].copy()\n",
    "    print(\"pase la mascara de juan de columnas\")\n",
    "    df_bussines.drop(columns=[\"is_open\",'attributes', 'hours','postal_code'], inplace=True)\n",
    "    categorias_deseadas = [\"restaurant\", \"bar\", \"pub\", \"cafe\", 'bakery', 'coffee shop', 'gym', 'gas station']\n",
    "\n",
    "    # Eliminar filas con valores NaN en la columna 'categories'\n",
    "    df_bussines = df_bussines.fillna(\"No Data\")\n",
    "    # Filtrar el DataFrame para mantener solo las filas que contienen al menos una de las categorías deseadas\n",
    "    df_bussines = df_bussines[df_bussines['categories'].str.contains('|'.join(categorias_deseadas), case=False)]\n",
    "    df_mascara= df_bussines[\"business_id\"]\n",
    "    \n",
    "\n",
    "    # Guardar el DataFrame como un archivo CSV\n",
    "    df_mascara.to_csv(nombre_archivo, index=False)\n",
    "    # Obtener el bucket\n",
    "    bucket = client.bucket(nombre_bucket)\n",
    "    # Subir el archivo al bucket\n",
    "    blob = bucket.blob(nombre_archivo)\n",
    "    blob.upload_from_filename(nombre_archivo)\n",
    "    print(f'DataFrame guardado en {ruta_archivo}')\n",
    "\n",
    "\n",
    "    return df_bussines\n",
    "\n",
    "def archivo_df_tip(file_uri):\n",
    "    df_tip = pd.read_json(file_uri, lines=True)\n",
    "    df_tip.drop(columns=\"compliment_count\", inplace=True)\n",
    "    for columna in df_tip.columns:\n",
    "        if df_tip[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_tip[columna] = df_tip[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_tip[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_tip[columna] = df_tip[columna].fillna(0)\n",
    "    return df_tip\n",
    "\n",
    "def archivo_df_user(file_uri):\n",
    "    df_user001 = pd.read_json(file_uri, lines=True)\n",
    "    df_user001.drop(columns=['elite','friends','compliment_hot','compliment_more', 'compliment_profile', 'compliment_cute','compliment_list', 'compliment_note', 'compliment_plain',\n",
    "        'compliment_cool', 'compliment_funny', 'compliment_writer','compliment_photos'], inplace=True)\n",
    "    for columna in df_user001.columns:\n",
    "        if df_user001[columna].dtype == 'object':\n",
    "            # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "            df_user001[columna] = df_user001[columna].fillna(\"No data\").str.strip()\n",
    "        elif df_user001[columna].dtype in ['int64', 'float64']:\n",
    "            # Si la columna es numérica, llenar los valores nulos con 0\n",
    "            df_user001[columna] = df_user001[columna].fillna(0)\n",
    "    print(\"completado etl archivo parquet\")\n",
    "    return df_user001\n",
    "\n",
    "def archivo_df_review(file_uri):\n",
    "    df_review = pd.read_json(file_uri, lines=True)\n",
    "    #for columna in df_review.columns:\n",
    "        #if df_review[columna].dtype == 'object':\n",
    "        # Si la columna es de texto, llenar los valores nulos con \"No data\" y eliminar espacios en blanco y saltos de línea\n",
    "        #    df_review[columna] = df_review[columna].fillna(\"No data\").str.strip()\n",
    "    #    if df_review[columna].dtype in ['int64', 'float64']:\n",
    "                    # Si la columna es numérica, llenar los valores nulos con 0\n",
    "    #        df_review[columna] = df_review[columna].fillna(0)\n",
    "    df_mascara=pd.read_csv(\"gs://yelp_dms/df_mascara.csv\")\n",
    "    print(\"carga de mascara completada\")\n",
    "    df_review['business_id'] = df_review['business_id'].astype(str)\n",
    "    df_mascara['business_id'] = df_mascara['business_id'].astype(str)\n",
    "\n",
    "    df_review = pd.merge(df_review, df_mascara, on='business_id', how='inner')\n",
    "    print(\"filtracion completa en archivo REVIEW\")\n",
    "    return df_review\n",
    "\n",
    "# Triggered by a change in a storage bucket\n",
    "@functions_framework.cloud_event\n",
    "def load_df(cloud_event):\n",
    "    data = cloud_event.data\n",
    "    event_id = cloud_event[\"id\"]\n",
    "    event_type = cloud_event[\"type\"]\n",
    "    print(f'Cloud event ID: {event_id}')\n",
    "    print(f'Event type: {event_type}')\n",
    "    bucket_name = data[\"bucket\"]\n",
    "    name = data[\"name\"]\n",
    "    print(f'Bucket: {bucket_name}')\n",
    "    print(f'File name: {name}')\n",
    "    # Create URI\n",
    "    file_uri = f'gs://{bucket_name}/{name}'\n",
    "    print(f'URI: {file_uri}')\n",
    "    # Obtener el objeto del bucket\n",
    "    bucket = cloud_storage.bucket(bucket_name)\n",
    "    # Obtener el blob (objeto) del archivo JSON en el bucket\n",
    "    blob = bucket.blob(name)\n",
    "    # Attempt to create DataFrame with URI\n",
    "    # Attempt to process DataFrame\n",
    "    print('Transformando dataframe...')\n",
    "    try:\n",
    "        if \"business\" in file_uri:\n",
    "            df_procesado = archivo_df_business(file_uri)\n",
    "            table_name = \"yelp-sites\"\n",
    "            print(\"dataframe cargado\") \n",
    "        if \"tip\" in file_uri:\n",
    "            df_procesado = archivo_df_tip(file_uri)\n",
    "            table_name = \"yelp-user-tips\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"user\" in file_uri:\n",
    "            df_procesado = archivo_df_user(file_uri)\n",
    "            table_name = \"yelp-user-info\"\n",
    "            print(\"dataframe cargado\")\n",
    "        if \"parte_\" in file_uri:\n",
    "            df_procesado=archivo_df_review(file_uri)\n",
    "            table_name=\"yelp-user-review\"\n",
    "            print(\"dataframe cargado\")\n",
    "        print('DataFrame Procesado')\n",
    "        print('Nuevas dimensiones:', df_procesado.shape)\n",
    "    except Exception as e:\n",
    "        # Cuando no procesa la información\n",
    "        print('DataFrame no pudo ser procesado: Error - ', e)\n",
    "        return None\n",
    "    # Insert to table\n",
    "    table_id = f'dms-pfh.yelp_data.{table_name}'\n",
    "    # Create job configurations\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        create_disposition='CREATE_IF_NEEDED',\n",
    "        write_disposition='WRITE_APPEND'\n",
    "        # source_format='NEWLINE_DELIMITED_JSON'\n",
    "    )\n",
    "    print('Job configuration created.')\n",
    "    # Requesting the job\n",
    "    load_job = bq.load_table_from_dataframe(\n",
    "        df_procesado,\n",
    "        table_id,\n",
    "        location='us-central1',\n",
    "        job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    print('Job finished')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
